{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.data import read_domain_ids_per_chain_from_txt\n",
    "from utils.dataset import *\n",
    "from diffusion_model.embed import *\n",
    "from diffusion_model.structure_diffusion_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCH = 100\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pdbs, train_pdb_chains = read_domain_ids_per_chain_from_txt('./data/train_domains.txt')\n",
    "test_pdbs, test_pdb_chains = read_domain_ids_per_chain_from_txt('./data/test_domains.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = BackboneCoordsDataLoader(train_pdb_chains, \"./data/train_backbone_coords_20.npy\", \"./data/train_data_res_20.npy\",seq_length=20, batch_size=128, shuffle=True)\n",
    "test_loader = BackboneCoordsDataLoader(test_pdb_chains, './data/test_backbone_coords_20.npy', './data/test_data_res_20.npy', seq_length=20, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = ProteinDiffusion(device=DEVICE)\n",
    "model = StructureModel().to(DEVICE)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 3.9112145165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.0223482962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 0.0206165601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3 Average loss: 0.0196833934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 4 Average loss: 0.0198371425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 Average loss: 0.0194038218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m R, t \u001b[39m=\u001b[39m rigidFrom3Points(n_coords, ca_coords, c_coords)\n\u001b[0;32m     12\u001b[0m q_0 \u001b[39m=\u001b[39m roma\u001b[39m.\u001b[39mrotmat_to_unitquat(R)\n\u001b[1;32m---> 13\u001b[0m single_repr \u001b[39m=\u001b[39m get_single_representation(pdb, res_label)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     15\u001b[0m pair_repr \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcdist(ca_coords, ca_coords, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Foward Diffusion\u001b[39;00m\n",
      "File \u001b[1;32md:\\Project\\Bioinfor_Project_1\\diffusion_model\\embed.py:36\u001b[0m, in \u001b[0;36mget_single_representation\u001b[1;34m(pdb_chain, res_label)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39m# Extract per-residue representations (on CPU)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 36\u001b[0m     results \u001b[39m=\u001b[39m model(batch_tokens, repr_layers\u001b[39m=\u001b[39;49m[\u001b[39m33\u001b[39;49m], return_contacts\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     37\u001b[0m token_representations \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mrepresentations\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m33\u001b[39m]\n\u001b[0;32m     39\u001b[0m \u001b[39m# Generate per-sequence representations\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39m# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\esm\\model\\esm1.py:156\u001b[0m, in \u001b[0;36mProteinBertModel.forward\u001b[1;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[0;32m    153\u001b[0m     padding_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m--> 156\u001b[0m     x, attn \u001b[39m=\u001b[39m layer(\n\u001b[0;32m    157\u001b[0m         x, self_attn_padding_mask\u001b[39m=\u001b[39;49mpadding_mask, need_head_weights\u001b[39m=\u001b[39;49mneed_head_weights\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m (layer_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39min\u001b[39;00m repr_layers:\n\u001b[0;32m    160\u001b[0m         hidden_representations[layer_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\esm\\modules.py:125\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[1;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[0;32m    123\u001b[0m residual \u001b[39m=\u001b[39m x\n\u001b[0;32m    124\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(x)\n\u001b[1;32m--> 125\u001b[0m x, attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[0;32m    126\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    127\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    128\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m    129\u001b[0m     key_padding_mask\u001b[39m=\u001b[39;49mself_attn_padding_mask,\n\u001b[0;32m    130\u001b[0m     need_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    131\u001b[0m     need_head_weights\u001b[39m=\u001b[39;49mneed_head_weights,\n\u001b[0;32m    132\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mself_attn_mask,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    134\u001b[0m x \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m x\n\u001b[0;32m    136\u001b[0m residual \u001b[39m=\u001b[39m x\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\esm\\multihead_attention.py:260\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[0;32m    258\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj(query)\n\u001b[0;32m    259\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(key)\n\u001b[1;32m--> 260\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(value)\n\u001b[0;32m    261\u001b[0m q \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaling\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias_k \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\huang\\.conda\\envs\\Pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (pdb, res_label, atom_coords) in enumerate(tqdm(train_loader, leave=False)):\n",
    "        # Data preparation\n",
    "        atom_coords = atom_coords.to(DEVICE)\n",
    "        atom_coords = atom_coords.to(torch.float32)\n",
    "        n_coords = atom_coords[:, :, 0]\n",
    "        ca_coords = atom_coords[:, :, 1]\n",
    "        c_coords = atom_coords[:, :, 2]\n",
    "        R, t = rigidFrom3Points(n_coords, ca_coords, c_coords)\n",
    "        q_0 = roma.rotmat_to_unitquat(R)\n",
    "        single_repr = get_single_representation(pdb, res_label).to(DEVICE)\n",
    "        \n",
    "        pair_repr = torch.cdist(ca_coords, ca_coords, p=2).to(torch.float32)\n",
    "        \n",
    "\n",
    "        # Foward Diffusion\n",
    "        batch_size = atom_coords.shape[0]\n",
    "        t = diffusion.sample_timesteps(batch_size = batch_size).to(DEVICE)\n",
    "        x_t = diffusion.coord_q_sample(ca_coords, t).to(torch.float32)\n",
    "        q_t = diffusion.quaternion_q_sample(q_0, t)\n",
    "\n",
    "        # train model\n",
    "        pred_coords = model(single_repr, pair_repr, q_t, x_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.mse_loss(pred_coords, ca_coords)\n",
    "        loss.backward() # calc gradients\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step() # backpropagation\n",
    "    print('====> Epoch: {} Average loss: {:.10f}'.format(epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
